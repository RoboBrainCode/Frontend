<br>
<h3 class="text-center">Our Research</h3>    
<br>
<div class="row text-justify">
  <div class="col-sm-8 col-sm-offset-2 col-xs-12">
    <p>
      In our multi-university open-source effort on building the Robo Brain,
      we are addressing research challenges in various domains:
      <ul>
        <li><a ng-href="#/research/?scrollTo=#research-ml">Machine Learning</a></li>
        <li><a ng-href="#/research/?scrollTo=#research-big-data">Large-Scale Data Processing</a></li>
        <li><a ng-href="#/research/?scrollTo=#research-language">Language and Dialog</a></li>
        <li><a ng-href="#/research/?scrollTo=#research-perception">Perception</a></li>
        <li><a ng-href="#/research/?scrollTo=#research-ai">AI and Reasoning Systems</a></li>
        <li><a ng-href="#/research/?scrollTo=#research-embodiment">Embodiment</a></li>
      </ul>
    </p>
    <p>
      To view our recent publications,
      <a ng-href="#/research/?scrollTo=#publications">please click here</a>.
    </p>
    <br><br>
  </div>
</div>
<div class="row text-justify">
  <div class="col-sm-6 col-xs-12">
    <ul class="list-unstyled">
      <li id="research-ml">
        <div class="panel panel-default feed-vision">
          <div class="panel-body">
            <br>
            <h4>Machine Learning</h4>
            <br>
            <p>
              The key in building our RoboBrain is to be able to
              learn from a variety of multi-modal data and from
              many types of learning signals. Some examples are:
              <ul>
                <li>
                  <em>Deep Learning</em>:
                   The large amounts of unlabeled data such as text,
                   images, 3D models allow us to use unsupervised deep
                   learning for learning good representations.
                </li>
                <li>
                  <em>Structured Learning and Discovering Latent Factors</em>:
                  There is a significant structure in the variety of data we have.
                  Using techniques such as graphical models and nonparametric learning
                  allows to model the relations such as spatial and semantic relations
                  between objects.
                </li>
                <li>
                  <em>Interactive Online Learning</em>:
                  This gives us an opportunity to develop new theoretically-sound
                  online learning algorithms that use the streaming data with weak labeling.
                </li>
              </ul>
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
      <li id="research-big-data">
        <div class="panel panel-default feed-affordance">
          <div class="panel-body">
            <br>
            <h4>Large-Scale Data Processing</h4>
            <br>
            <p>
              A robot has to learn from multi-modal data sources such as videos,
              images, language and even physical interaction with humans.
              This necessitates building a large-scale knowledge graph
              that allows efficient data storage, probabilistic queries and retrieval.
              Another challenge is the variety of learning algorithms the system should support,
              from large-scale offline learning to real-time online learning.
            </p>
            <p>
              This multi-modality of data and the large probabilistic query space makes this
              problem more challenging than existing large-scale systems that typically store single
              type of data source such as images, or textual data. 
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
      <li id="research-language">
        <div class="panel panel-default feed-planning">
          <div class="panel-body">
            <br>
            <h4>Language and Dialog</h4>
            <br>
            <p>
              For seamless human-robot collaboration robots should be able to interact with
              humans using natural language. Robots should not only parse the language,
              but also ground its meaning into suitable actions based on the context and the environment.
            </p>
            <p>
              This require building learning algorithms that can model the ambiguity in language,
              the uncertainty in the robotic actions, and the implicit representation of the
              task and the environment. This will require exploring new ways for collecting
              natural language data from humans, and designing algorithms for mapping the
              language to robot actions obeying environment constraints (the laws of physics).
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
    </ul>
  </div>
  <div class="col-sm-6 col-xs-12">
    <ul class="list-unstyled">
      <li id="research-perception">
        <div class="panel panel-default feed-object-usage">
          <div class="panel-body">
            <br>
            <h4>Perception (3D Images and Videos)</h4>
            <br>
            <p>
              In order to act in the environment, a robot should be able understand its
              surrounding  objects, furniture layout, humans, road-signs, etc.
              This requires building data-driven vision algorithms that run in real-time. 
            </p>
            <p>
              The challenges encountered here are far richer than traditional computer vision because:
              <ul>
                <li>
                  The perception is for <em>action</em>, in that the agent constantly interacts
                    with the environment and modifies it.
                </li>
                <li>
                  Agent can use a variety of perception signals such as 2D images,
                  videos, as well as 3D point-cloud data from RGBD and LIDAR sensors.  
                </li>
              </ul>
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
      <li id="research-ai">
        <div class="panel panel-default feed-3D-detection">
          <div class="panel-body">
            <br>
            <h4>AI and Reasoning Systems</h4>
            <br>
            <p>
              Robots need to reason about the constraints and behavior of the agents
              in the real world. We are developing methods that model other human's
              intentions, methods that learn constraints by observing events in the environments.
              Unlike other traditional rule-based systems, the challenge here is to learn and
              probabilistically integrate such knowledge into our Robo Brain.
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
      <li id="research-embodiment">
        <div class="panel panel-default feed-vision">
          <div class="panel-body">
            <br>
            <h4>Embodiment (Robot Applications)</h4>
            <br>
            <p>
              We are using our Robo Brain in various robots such as Baxter,
              PR2, and several others. Since every robot is physically different and
              operates in a variety of environments, we need to build methods that allow
              our Robo Brain to be embodied in different robots and for different tasks.  
            </p>
            <br>
          </div>
        </div>
        <br>
      </li>
    </ul>
  </div>
</div>
<br>
<h4 id="publications">Our Publications</h4>
<br>
<ul class="list-unstyled">
  <li ng-repeat="p in publications">
    <strong>
      <a ng-href="{{ p.url }}" target="_blank">{{ p.title }}</a>
    </strong>
    <br>
    {{ p.authors }}
    <br>
    <em>{{ p.collection }}</em>, {{ p.year }}
    <br>
    <div ng-if="p.awards">
      <strong>{{ p.awards }}</strong>
    </div>
    <div ng-if="p.misc">
      <em><small>{{ p.misc }}</small></em>
    </div>
    <br>
  </li>
</ul>
<br><br>